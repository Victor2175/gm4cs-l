{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b261e8",
   "metadata": {},
   "source": [
    "# Tutorial: Linear Variational Autoencoder (VAE) for Climate Data\n",
    "\n",
    "This tutorial demonstrates the training of a Linear Variational Autoencoder (VAE) to learn transformations from input climate data to corresponding forced responses. It includes data preprocessing, model training, and evaluation using Leave-One-Out (LOO) cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261dde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /Users/lharriso/Documents/GitHub/gm4cs-l/data\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add utility paths\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "\n",
    "# Import utility functions\n",
    "from utils.data_loading import *\n",
    "from utils.data_processing import *\n",
    "from utils.trend_vae_3 import *\n",
    "from utils.animation import *\n",
    "from utils.metrics import *\n",
    "from utils.pipeline import *\n",
    "\n",
    "# Enable autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Define data path\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, 'data')\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ec2b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Use MPS / Cuda or CPU if none of the options are available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5454a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/lharriso/Documents/GitHub/gm4cs-l/data/ssp585_time_series.pkl\n",
      "Data loaded successfully.\n",
      "Filtering data...\n",
      "Data loaded successfully.\n",
      "Filtering data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 62549.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data filtered. Kept 34 models\n",
      "Creating NaN mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:02<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN mask created.\n",
      "Masking out NaN values...\n",
      "Masking out NaN values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:01<00:00, 22.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values masked out.\n",
      "Reshaping data...\n",
      "Reshaping data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:04<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped.\n",
      "Adding the forced response to the data...\n",
      "Adding the forced response to the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:27<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced response added.\n",
      "Removing NaN values from the grid...\n",
      "Removing NaN values from the grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:03<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/9v/q6t68ds12f9c4dgn9gnwzrmr0000gn/T/ipykernel_93349/209886894.py:7: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  model_keys = random.sample(data.keys(), n_models)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 34 models in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "filename = os.path.join(data_path, 'ssp585_time_series.pkl')\n",
    "data, nan_mask = preprocess_data(data_path, filename)\n",
    "\n",
    "# Randomly select and keep the data corresponding to n models\n",
    "n_models = 34\n",
    "model_keys = random.sample(data.keys(), n_models)\n",
    "data = {key: value for key,value in data.items() if key in model_keys}\n",
    "\n",
    "# Select one of the models randomly for testing and the rest for training according to the leave-one-out strategy\n",
    "models = list(data.keys())\n",
    "print(f\"There are {len(models)} models in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4cb99",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca9e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Normalizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33/33 [00:25<00:00,  1.30it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalization completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 33/33 [00:00<00:00, 49030.12it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 1/1 [00:00<00:00, 1971.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: torch.Size([165, 6523])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MSE scores saved to /Users/lharriso/Documents/GitHub/gm4cs-l/mse_scores.pkl\n"
     ]
    }
   ],
   "source": [
    "# Leave-One-Out Cross-Validation\n",
    "mse_scores = []\n",
    "center = True  # Center the data\n",
    "hidden_dim = 128 # Increased for better representation\n",
    "feat_dim = 6523\n",
    "latent_dim = 64 # Intermediate layer size\n",
    "z_dim = 5 # Actual latent space dimension\n",
    "trend_poly = 2\n",
    "seq_len = 165 # Number of time steps in the sequence\n",
    "batch_size = 32 # Batch size for training\n",
    "\n",
    "for test_model in data.keys():\n",
    "    # Split data into training and testing sets\n",
    "    train_models = [model for model in data.keys() if model != test_model]\n",
    "    train_data = {model: data[model] for model in train_models}\n",
    "    test_data = {test_model: data[test_model]}\n",
    "\n",
    "    # Normalize the data\n",
    "    normalized_train_data, normalized_test_data, _, _ = normalize_data(train_data, test_data, center=center)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = ClimateDataset(normalized_train_data)\n",
    "    test_dataset = ClimateDataset(normalized_test_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = train_dataset[0]['input'][1]\n",
    "    vae_model = Trend_Vae(\n",
    "        seq_len=seq_len,\n",
    "        feat_dim=feat_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        latent_dim=latent_dim,\n",
    "        z_dim=z_dim,\n",
    "        trend_poly=trend_poly,\n",
    "        use_residual_conn=True,\n",
    "        device=device,\n",
    "        ).to(device)\n",
    "\n",
    "    # Train the model (simplified for tutorial purposes)\n",
    "    epochs = 500\n",
    "    losses = []\n",
    "\n",
    "    optimizer = torch.optim.Adam(vae_model.parameters(), lr=3e-4) # Weight decay is not used in this version (was weight_decay=1e-5)\n",
    "\n",
    "    losses = train_vae(vae_model, train_loader, optimizer, epochs=epochs, device=device)\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = evaluate_vae(vae_model, test_loader, device, testing_statistics=None)  # Replace None with actual testing_statistics if available\n",
    "    mse = results['mse']\n",
    "    normalized_mse = results['normalized_mse']\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    print(f\"Test model: {test_model}, MSE: {mse}, Normalized MSE: {normalized_mse}\")\n",
    "\n",
    "# Save MSE scores to a file\n",
    "mse_file = os.path.join(current_dir, 'mse_scores.pkl')\n",
    "with open(mse_file, 'wb') as f:\n",
    "    pkl.dump(mse_scores, f)\n",
    "\n",
    "print(f\"MSE scores saved to {mse_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6802069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MSE Distributions\n",
    "# Load MSE scores\n",
    "with open(mse_file, 'rb') as f:\n",
    "    mse_scores = pkl.load(f)\n",
    "\n",
    "# Create a quantile plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.ecdfplot(mse_scores, complementary=True)\n",
    "plt.title('Quantile Plot of MSE Distributions')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('1 - CDF')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
