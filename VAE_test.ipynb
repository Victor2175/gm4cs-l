{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder\n",
    "This notebook demonstrates the training of a Variational Autoencoder (VAE) to learn transformations from input climate data to corresponding forced responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path: /Users/lharriso/Documents/GitHub/gm4cs-l/data\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add utility paths\n",
    "sys.path.append(os.path.join(os.getcwd(), 'utils'))\n",
    "\n",
    "# Import utility functions\n",
    "from utils.data_loading import *\n",
    "from utils.data_processing import *\n",
    "from utils.vae import *\n",
    "from utils.animation import *\n",
    "from utils.metrics import *\n",
    "from utils.pipeline import *\n",
    "\n",
    "# Enable autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Define data path\n",
    "current_dir = os.getcwd()\n",
    "data_path = os.path.join(current_dir, 'data')\n",
    "print(f\"Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Use MPS / Cuda or CPU if none of the options are available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/lharriso/Documents/GitHub/gm4cs-l/data/ssp585_time_series.pkl\n",
      "Data loaded successfully.\n",
      "Filtering data...\n",
      "Data loaded successfully.\n",
      "Filtering data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:00<00:00, 59954.32it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 59954.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data filtered. Kept 34 models\n",
      "Creating NaN mask...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 19.58it/s]\n",
      "100%|██████████| 34/34 [00:01<00:00, 19.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN mask created.\n",
      "Masking out NaN values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 22.22it/s]\n",
      "100%|██████████| 34/34 [00:01<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values masked out.\n",
      "Reshaping data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:03<00:00,  9.56it/s]\n",
      "100%|██████████| 34/34 [00:03<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped.\n",
      "Adding the forced response to the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:05<00:00,  6.50it/s]\n",
      "100%|██████████| 34/34 [00:05<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forced response added.\n",
      "Removing NaN values from the grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:01<00:00, 30.78it/s]\n",
      "100%|██████████| 34/34 [00:01<00:00, 30.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values removed.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "filename = os.path.join(data_path, 'ssp585_time_series.pkl')\n",
    "data, nan_mask = preprocess_data(data_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9v/q6t68ds12f9c4dgn9gnwzrmr0000gn/T/ipykernel_58767/3740862327.py:3: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  model_keys = random.sample(data.keys(), n)\n"
     ]
    }
   ],
   "source": [
    "# Randomly select and keep the data corresponding to n models\n",
    "n = 5\n",
    "model_keys = random.sample(data.keys(), n)\n",
    "data = {key: value for key,value in data.items() if key in model_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models: ['EC-Earth3', 'E3SM-2-0', 'GISS-E2-1-G', 'ACCESS-ESM1-5']\n",
      "Testing model: GISS-E2-2-G\n"
     ]
    }
   ],
   "source": [
    "# Select one of the models randomly for testing and the rest for training according to the leave-one-out strategy\n",
    "test_model = random.choice(list(data.keys()))\n",
    "train_models = [model for model in data.keys() if model != test_model]\n",
    "\n",
    "# Create the training and testing datasets\n",
    "train_data = {model: data[model] for model in train_models}\n",
    "test_data = {test_model: data[test_model]}\n",
    "\n",
    "print(f\"Training models: {train_models}\")\n",
    "print(f\"Testing model: {test_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 4/4 [00:00<00:00, 102300.10it/s]\n",
      "\n",
      "/Users/lharriso/Documents/GitHub/gm4cs-l/utils/vae.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  self.inputs = torch.tensor(self.inputs, dtype=torch.float32)\n",
      "/Users/lharriso/Documents/GitHub/gm4cs-l/utils/vae.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  self.inputs = torch.tensor(self.inputs, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 1/1 [00:00<00:00, 35848.75it/s]\n",
      "Processing models: 100%|██████████| 1/1 [00:00<00:00, 35848.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 124\n",
      "Testing dataset size: 11\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "train_dataset = ClimateDataset(train_data)\n",
    "test_dataset = ClimateDataset(test_data)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f'Training dataset size: {len(train_dataset)}')\n",
    "print(f'Testing dataset size: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:08,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:01<00:07,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:02<00:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:03<00:05,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize the VAE model\n",
    "input_dim = train_dataset.inputs.shape[1] * train_dataset.inputs.shape[2]  # Flattened input dimensions\n",
    "latent_dim = 100\n",
    "hidden_dim = 50\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "vae_model = VAE(input_dim=input_dim, hidden_dim = hidden_dim, latent_dim=latent_dim, device=device).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the VAE\n",
    "train_vae(vae_model, train_loader, optimizer, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
